{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "ijmpgYnKYklI",
        "y-Ehk30pYrdP",
        "-Kee-DAl2viO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aamir047/Unsupervised-ML---Netflix-Movies-and-TV-Shows-Clustering.ipynb/blob/main/Unsupervised_ML_Netflix_Movies_and_TV_Shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Unsupervised ML - Netflix Movies and TV Shows Clustering\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Aamir Hussain\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This capstone project focused on end to end Machine Learning for the Netflix Movies and TV Shows dataset, aiming to uncover insights about the platform catalog and build a reliable model to classify content as either Movie or TV Show. The project followed a structured approach, moving from data preprocessing and exploration to feature engineering, modeling, evaluation, and deployment readiness.\n",
        "\n",
        "Dataset Overview\n",
        "\n",
        "The dataset included 7,787 titles with attributes such as type, title, director, cast, country, release year, rating, duration, listed genres, and description. Initial analysis highlighted class imbalance, with Movies far outnumbering TV Shows. Missing values were found in date_added and duration, which were treated using imputation and feature transformations.\n",
        "\n",
        "Data Wrangling and Preprocessing\n",
        "\n",
        "Key transformations were performed to make the data ML-ready:\n",
        "\n",
        "Handling missing values and outliers in duration and date fields.\n",
        "\n",
        "Feature engineering such as extracting duration_num, is_movie, primary_genre, and content addition year/month.\n",
        "\n",
        "Categorical encoding using a hybrid approach: one-hot encoding for ratings, frequency encoding for high-cardinality features like director/cast, and multi-label binarization for genres.\n",
        "\n",
        "Text preprocessing for descriptions: contraction expansion, lowercasing, punctuation and stopword removal, lemmatization, and tokenization. This produced normalized text used for feature extraction.\n",
        "\n",
        "Text vectorization via TF-IDF, reducing thousands of words into weighted numeric features that captured both frequency and importance.\n",
        "\n",
        "Exploratory Data Analysis\n",
        "\n",
        "EDA revealed that Movies dominate the Netflix catalog, with most lasting around 90–120 minutes, while TV Shows are generally described by terms like “season” and “series.” Geographically, the U.S. and India contributed a large share of content. Visualizations across univariate, bivariate, and multivariate dimensions helped identify patterns such as release year trends, genre popularity, and content growth over time.\n",
        "\n",
        "Model Building and Evaluation\n",
        "\n",
        "Three ML models were implemented and compared:\n",
        "\n",
        "Logistic Regression – served as a baseline. While simple and interpretable, it favored Movies due to class imbalance, achieving ~73% accuracy but weak recall for TV Shows.\n",
        "\n",
        "Random Forest Classifier – improved balance by capturing nonlinear patterns. With hyperparameter tuning (RandomizedSearchCV), accuracy rose to ~78%, with stronger recall and F1 for TV Shows. This model offered the best trade-off between accuracy and fairness.\n",
        "\n",
        "XGBoost Classifier – powerful and optimized for boosting weak learners. It excelled at identifying Movies with very high recall (~92%) but initially struggled with TV Shows (~34% recall). After tuning, recall for TV Shows improved to ~47%, but Random Forest remained more balanced.\n",
        "\n",
        "Evaluation metrics considered were accuracy, precision, recall, and F1-score. For business impact, recall and F1-score for TV Shows were prioritized, as improving minority class detection ensures Netflix recommendations are fair, diverse, and engaging for all users.\n",
        "\n",
        "Model Explainability and Deployment\n",
        "\n",
        "To ensure interpretability, feature importance was used. Words like “season, series” strongly predicted TV Shows, while “minutes, film, story” were indicative of Movies. This confirmed that the models were learning meaningful patterns from the data. The best model, Random Forest, was saved in pickle format for deployment, making it ready to integrate into APIs or recommendation systems.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The project demonstrated how structured ML pipelines can turn raw streaming metadata into actionable insights. Random Forest Classifier (tuned) was chosen as the final model due to its balanced accuracy (78%) and improved recall for TV Shows, which was critical for reducing bias toward Movies. From a business standpoint, this ensures Netflix can deliver more relevant, fair, and diverse recommendations, ultimately improving user satisfaction and retention."
      ],
      "metadata": {
        "id": "ogJKJt-QSqJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ILq1Vq5jinZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "!pip install contractions\n",
        "# Core Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Text Preprocessing & Feature Extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Clustering Algorithms\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Dimensionality Reduction (for Visualization)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Preprocessing Utilities\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# For handling warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display Settings (better view of dataframes)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1zgqDbHJjt2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "df_netflix = pd.read_csv('/content/drive/MyDrive/Machine Learning /Netflix ML DataSet/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "#Viewing the first 5 rows of the dataset\n",
        "\n",
        "df_netflix.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "rows, cols = df_netflix.shape\n",
        "print(f\"\\n Dataset contains {rows} rows and {cols} columns.\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "df_netflix.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "print(\"\\n Duplicate Rows:\", df_netflix.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "print(\"\\n Total Null Values :\", df_netflix.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_netflix.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()\n",
        "\n",
        "#As we can see director/Cast has the most missing value followed by country and then rating"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Bar plot of missing values count\n",
        "missing_counts = df_netflix.isnull().sum()\n",
        "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=missing_counts.index, y=missing_counts.values, palette=\"magma\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Number of Missing Values\")\n",
        "plt.title(\"Missing Values Count per Column\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r2iF4LIYmaNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains Netflix Movies & TV Shows metadata with around 7,800+ rows and 12 columns.\n",
        "\n",
        "Columns include details like type (Movie/TV Show), title, director, cast, country, release year, rating, duration, genres, and description.\n",
        "\n",
        "Missing values are present, mainly in director, cast, country, date_added, and rating.\n",
        "\n",
        "Movies and TV Shows can be analyzed separately since duration represents minutes for movies and seasons for TV shows.\n",
        "\n",
        "Data is well-suited for exploring content distribution (by year, genre, country, rating) and for building recommendation or clustering models later."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "# Display dataset columns\n",
        "print(\"Columns in Netflix dataset:\")\n",
        "print(df_netflix.columns.tolist())\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "# For numerical columns\n",
        "print(\"Numerical Summary:\")\n",
        "print(df_netflix.describe())\n",
        "\n",
        "# For categorical columns\n",
        "print(\"\\n Categorical Summary:\")\n",
        "print(df_netflix.describe(include='object'))"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A breif Variables Description\n",
        "\n",
        "show_id - Unique identifier for each title\n",
        "\n",
        "type - Movie or TV Show\n",
        "\n",
        "title - Name of the content\n",
        "\n",
        "director - Director (many missing)\n",
        "\n",
        "cast - Main actors (many missing)\n",
        "\n",
        "country - Country of production\n",
        "\n",
        "date_added - Date added to Netflix\n",
        "\n",
        "release_year - Year of release\n",
        "\n",
        "rating - Audience rating (e.g., TV-MA, PG)\n",
        "\n",
        "duration - Minutes (for Movies) / Seasons (for TV Shows)\n",
        "\n",
        "listed_in - Genre(s)\n",
        "\n",
        "description - Short summary"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "# Unique values count for each column\n",
        "for col in df_netflix.columns:\n",
        "    print(f\"{col} - {df_netflix[col].nunique()} unique values\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Copy of dataset for wrangling\n",
        "df = df_netflix.copy()\n",
        "\n",
        "# 1. Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# 2. Handling Missing Values\n",
        "# Replace missing values with 'Unknown' for categorical columns\n",
        "for col in ['director', 'cast', 'country', 'rating']:\n",
        "    df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "# Replace missing dates with 'Unknown'\n",
        "df['date_added'] = df['date_added'].fillna('Unknown')\n",
        "\n",
        "# 3. Clean duration column\n",
        "# Duration is in minutes for Movies and seasons for TV Shows\n",
        "df['duration'] = df['duration'].str.replace(' min', '').str.replace(' Season', '').str.replace(' Seasons', '')\n",
        "df['duration'] = pd.to_numeric(df['duration'], errors='coerce')\n",
        "\n",
        "# 4. Convert date_added to datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "# 5. Extract year and month from date_added\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "\n",
        "# 6. Handle duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 7. Reset index after cleaning\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#  Final Cleaned Dataset Overview\n",
        "print(\"Rows:\", df.shape[0], \" | Columns:\", df.shape[1])\n",
        "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Manipulations Done\n",
        "\n",
        "Standardized column names - lowercase, replaced spaces with\n",
        "underscores.\n",
        "\n",
        "Missing values handling - filled director, cast, country, rating with \"Unknown\".\n",
        "\n",
        "duration cleaning - removed \"min\" / \"Seasons\", converted to numeric.\n",
        "\n",
        "date_added - converted to datetime, extracted year_added & month_added.\n",
        "\n",
        "Removed duplicates - kept only unique records.\n",
        "\n",
        "Reset index - cleaned dataset shape: 7787 rows & 14 columns.\n",
        "\n",
        "Insights Found (So Far)\n",
        "\n",
        "Dataset has a balanced mix of Movies & TV Shows, but Movies dominate.\n",
        "\n",
        "Some features (director, cast, country) had many missing values → replaced with \"Unknown\".\n",
        "\n",
        "98 records dont have a date_added, meaning we dont know when they came on Netflix.\n",
        "\n",
        "802 records lack valid duration info (mostly TV Shows without season details).\n",
        "\n",
        "Dataset is now clean, but we’ll still need to handle the remaining missing values carefully depending on analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Countplot for Movies vs TV Shows\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(data=df_netflix, x='type', palette=\"Set2\")\n",
        "\n",
        "# Add labels\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f\"{p.get_height()}\",\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=10, color='black', xytext=(0,5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "plt.title(\"Distribution of Movies vs TV Shows\", fontsize=14)\n",
        "plt.xlabel(\"Type of Content\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why This Chart :\n",
        "\n",
        "Simple count comparison between Movies and TV Shows.\n",
        "\n",
        "Helps us see the content focus of Netflix.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "Insights:\n",
        "\n",
        "Netflix has more Movies than TV Shows in the dataset.\n",
        "\n",
        "This shows Netflix is still movie-heavy, but TV Shows are a growing segment.\n"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact:\n",
        "\n",
        "Positive: Movies attract casual viewers (binge-watch a single title).\n",
        "\n",
        "Positive: TV Shows create long-term engagement since users return for multiple episodes/seasons.\n",
        "\n",
        "Netflix needs a balance of both to sustain growth."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Plot ratings distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "ax = sns.countplot(data=df_netflix, y='rating', order=df_netflix['rating'].value_counts().index, palette=\"coolwarm\")\n",
        "\n",
        "# Add labels\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f\"{p.get_width()}\",\n",
        "                (p.get_width() + 50, p.get_y() + p.get_height() / 2.),\n",
        "                ha='center', va='center', fontsize=9, color='black')\n",
        "\n",
        "plt.title(\"Distribution of Content Ratings\", fontsize=14)\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings define audience targeting (Kids, Teens, Adults).\n",
        "\n",
        "Horizontal barplot is better since many categories exist and we want to compare counts clearly."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most frequent ratings are TV-MA (Mature Audience) and TV-14 (Teens + Adults).\n",
        "\n",
        "Very few titles are rated G (General Audience) or TV-Y (Kids)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Shows Netflix caters largely to adults and teenagers, the biggest subscriber segment.\n",
        "\n",
        "Negative: Weak kids’ content library compared to Disney+ or Amazon Kids → potential gap in market."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Content count by release year\n",
        "release_trend = df_netflix['release_year'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(release_trend.index, release_trend.values, marker='o', linestyle='-', color='royalblue')\n",
        "\n",
        "plt.title(\"Trend of Content Release by Year\", fontsize=14)\n",
        "plt.xlabel(\"Release Year\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line chart is best to show growth trend over time.\n",
        "\n",
        "Helps us see how Netflix’s content library evolved across years."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sharp rise in content releases after 2015, peaking around 2018–2020.\n",
        "\n",
        "Drop in 2021–2022 possibly due to COVID-19 production delays."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Netflix heavily expanded during 2015–2020, increasing global dominance.\n",
        "\n",
        "Negative: Decline post-2020 may signal production slowdown - risk of subscriber churn if not addressed."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Split country column (some rows have multiple countries)\n",
        "from collections import Counter\n",
        "\n",
        "country_series = df_netflix['country'].dropna().astype(str).apply(lambda x: x.split(\",\"))\n",
        "country_list = [c.strip() for sublist in country_series for c in sublist]\n",
        "\n",
        "# Get top 10 countries\n",
        "top_countries = Counter(country_list).most_common(10)\n",
        "countries, counts = zip(*top_countries)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=list(counts), y=list(countries), palette=\"viridis\")\n",
        "\n",
        "plt.title(\"Top 10 Countries Producing Netflix Content\", fontsize=14)\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Country\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart is perfect to compare top producers by count.\n",
        "\n",
        "Countries are sorted → easy ranking interpretation."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "United States dominates content production.\n",
        "\n",
        "India, UK, Japan, South Korea are strong secondary contributors.\n",
        "\n",
        "Non-English countries like France, Canada, Spain also make the list."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Shows Netflix’s global reach, leveraging local industries.\n",
        "\n",
        "Negative: High dependency on U.S. content - risk if U.S. market growth saturates."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "#Top 10 Directors with Most Titles\n",
        "\n",
        "top_directors = df_netflix['director'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_directors.values, y=top_directors.index, palette=\"mako\")\n",
        "plt.title(\"Top 10 Directors on Netflix\", fontsize=14)\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Director\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directors are creators; identifying top contributors shows who Netflix relies on."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few directors dominate Netflix’s catalog (e.g., Indian, US directors appear often). Netflix relies on recurring creators for content."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Positive - Netflix builds long-term partnerships with proven directors (brand loyalty). Negative - over-reliance on a small set of creators can reduce content diversity."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "#Content Added per Year\n",
        "\n",
        "yearly_added = df['year_added'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(yearly_added.index, yearly_added.values, color=\"teal\")\n",
        "plt.title(\"Content Added to Netflix Over the Years\", fontsize=14)\n",
        "plt.xlabel(\"Year Added\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps understand Netflix’s growth strategy and release patterns."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surge in titles after 2015, peak between 2018–2020. Slower growth afterward"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - Netflix aggressively scaled its catalog.\n",
        "Negative - post-2020 decline could hint at cost-cutting or content saturation."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "#Distribution of Ratings\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(data=df_netflix, x=\"rating\", order=df_netflix['rating'].value_counts().index, palette=\"pastel\")\n",
        "plt.title(\"Distribution of Ratings\", fontsize=14)\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content rating defines audience segments (Kids, Teens, Adults)."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority of titles are TV-MA, TV-14, and R, i.e., mature audience focus."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - Netflix’s key subscribers are adults.\n",
        "Negative - Less kids-friendly content risks losing the family audience segment."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "#Distribution of Movie Durations\n",
        "\n",
        "movies = df_netflix[df_netflix['type'] == \"Movie\"]\n",
        "movies['duration_int'] = movies['duration'].str.replace(\" min\",\"\").astype(float)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(movies['duration_int'], bins=30, kde=True, color=\"coral\")\n",
        "plt.title(\"Distribution of Movie Durations\", fontsize=14)\n",
        "plt.xlabel(\"Duration (minutes)\")\n",
        "plt.ylabel(\"Number of Movies\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duration affects watch completion rates and customer satisfaction."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most movies last 80–120 minutes (standard cinema length). Few very short/long titles."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - Audience-friendly runtimes maximize engagement.\n",
        "Negative - lack of short-form movies may limit appeal for mobile-first users."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "#Distribution of TV Show Seasons\n",
        "\n",
        "tv_shows = df_netflix[df_netflix['type'] == \"TV Show\"]\n",
        "tv_shows['seasons_int'] = tv_shows['duration'].str.replace(\" Season\",\"\").str.replace(\"s\",\"\").astype(float)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x=tv_shows['seasons_int'], order=tv_shows['seasons_int'].value_counts().index, palette=\"Set2\")\n",
        "plt.title(\"Distribution of TV Show Seasons\", fontsize=14)\n",
        "plt.xlabel(\"Number of Seasons\")\n",
        "plt.ylabel(\"Number of TV Shows\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of seasons shows Netflix’s production strategy (short vs long series)."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most shows have only 1 season, reflecting Netflix’s “limited series” strategy."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - Lower cost per show, binge-friendly.\n",
        "Negative - Some audiences prefer long-running franchises (HBO-style loyalty)."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "#Top 10 Genres\n",
        "\n",
        "genre_series = df_netflix['listed_in'].dropna().apply(lambda x: x.split(\",\"))\n",
        "genre_list = [g.strip() for sublist in genre_series for g in sublist]\n",
        "\n",
        "top_genres = Counter(genre_list).most_common(10)\n",
        "genres, counts = zip(*top_genres)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=list(counts), y=list(genres), palette=\"flare\")\n",
        "plt.title(\"Top 10 Genres on Netflix\", fontsize=14)\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Genre\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genre mix shows what kind of stories Netflix invests in."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dramas, Documentaries, International TV dominate. Heavy international focus."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - Global reach, cultural diversity.\n",
        "Negative - US audience may feel underserved compared to Hollywood-centric competitors."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "#Top 10 Countries by Content\n",
        "top_countries = df_netflix['country'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_countries.values, y=top_countries.index, palette=\"crest\")\n",
        "plt.title(\"Top 10 Countries Producing Netflix Content\", fontsize=14)\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Country\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Country analysis shows Netflix’s global production footprint."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "USA leads by far, followed by India, UK, and other international hubs."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - Localized content boosts global subscriptions.\n",
        "Negative - Heavy US dependence might limit cultural diversity."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "#Word Cloud of Titles\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = \" \".join(df_netflix['title'])\n",
        "wordcloud = WordCloud(width=1000, height=500, background_color=\"black\", colormap=\"Set2\").generate(text)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most Common Words in Netflix Titles\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word clouds visualize common keywords in content titles."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words like Love, Life, Story, World, Dark appear frequently."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - Netflix targets universal themes.\n",
        "Negative - Risk of repetitiveness in branding/content identity."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "#Word Cloud of Content Genres (from listed_in)\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all genres into one string\n",
        "text = \" \".join(df_netflix['listed_in'].dropna().astype(str))\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=1200, height=600, background_color=\"black\", colormap=\"plasma\").generate(text)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most Common Genres on Netflix\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word clouds give a visual representation of frequency in text data.\n",
        "\n",
        "In this case, we instantly see which genres/categories Netflix focuses on (e.g., International Movies, Dramas, Comedies).\n",
        "\n",
        "This complements the bar charts by offering a more engaging, storytelling-driven view."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word cloud highlights that “International Movies,” “Dramas,” “Comedies,” and “TV Shows” are among the most frequently listed categories on Netflix.\n",
        "Genres like Documentaries, Stand-Up Comedy, and Action also stand out but appear less prominently compared to the core categories.\n",
        "\n",
        "This shows that Netflix’s catalog is heavily skewed toward dramas and international content, aligning with its strategy of expanding beyond the U.S. to attract a global audience."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps Netflix marketing and content teams identify top genres to emphasize in promotions.\n",
        "\n",
        "Useful for understanding consumer demand patterns (if “International Dramas” is huge, Netflix can invest more in those regions).\n",
        "\n",
        "Aids strategic planning for content acquisition."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "numeric_df = df[['release_year','year_added','month_added']]\n",
        "corr = numeric_df.corr()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation shows how numeric variables relate."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong positive correlation between year_added and release_year (newer content gets added quickly)."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "sns.pairplot(numeric_df, diag_kind=\"kde\", corner=True)\n",
        "plt.suptitle(\"Pair Plot of Numeric Variables\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plots reveal variable distributions & relationships simultaneously."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributions are skewed toward recent years; strong diagonal patterns between release and addition years."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content Type vs. Duration\n",
        "\n",
        "Null (H0): There is no difference in average duration between Movies and TV Shows.\n",
        "\n",
        "Alternative (H1): Movies and TV Shows differ significantly in their average duration."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "# Separate Movies and TV Shows\n",
        "movies = df_netflix[df_netflix['type'] == 'Movie'].copy()\n",
        "shows = df_netflix[df_netflix['type'] == 'TV Show'].copy()\n",
        "\n",
        "# Clean 'duration' column\n",
        "movies['duration_int'] = movies['duration'].str.replace(\" min\", \"\").astype(float)\n",
        "shows['duration_int'] = shows['duration'].str.replace(\" Season\", \"\").str.replace(\"s\",\"\").astype(float)\n",
        "\n",
        "# Drop missing values\n",
        "movies = movies.dropna(subset=['duration_int'])\n",
        "shows = shows.dropna(subset=['duration_int'])\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_val = ttest_ind(movies['duration_int'], shows['duration_int'], equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_val)\n",
        "\n",
        "# Interpretation\n",
        "if p_val < 0.05:\n",
        "    print(\"Reject Null Hypothesis: Significant difference in average duration between Movies and TV Shows.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference found.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation\n",
        "\n",
        "If p < 0.05, then Movies and TV Shows have significantly different average durations.\n",
        "\n",
        "If p ≥ 0.05, then there’s no evidence of a difference."
      ],
      "metadata": {
        "id": "lcOHpJkQ-ycL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothetical Statement 1 (Content Type vs Duration), the statistical test I used is:\n",
        "\n",
        "Independent Two-Sample t-test (Welch’s t-test)\n",
        "\n",
        "Why this test?\n",
        "\n",
        "We are comparing the mean duration between two independent groups:\n",
        "\n",
        "Movies (measured in minutes)\n",
        "\n",
        "TV Shows (measured in seasons)\n",
        "\n",
        "Their variances may not be equal, so we use Welch’s t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Welch’s independent t-test because it is the most suitable statistical test to compare the mean difference between two independent groups (Movies vs TV Shows) with potentially unequal variances."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis: Content Release Year vs. Content Type\n",
        "\n",
        "Null Hypothesis (H0): The proportion of Movies and TV Shows released is the same across years.\n",
        "\n",
        "Alternative Hypothesis (H1): The proportion of Movies and TV Shows released differs significantly across years."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = pd.crosstab(df_netflix['release_year'], df_netflix['type'])\n",
        "\n",
        "# Perform Chi-Square Test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-Square Test Results\")\n",
        "print(f\"Chi2 Statistic: {chi2:.2f}\")\n",
        "print(f\"p-value: {p:.6f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "#In a Chi-Square test of independence, the degrees of freedom represent how many values in the contingency table can vary independently once the totals are fixed."
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Chi-Square Test of Independence to obtain the p-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both variables (release_year and type) are categorical.\n",
        "\n",
        "We want to test if there is an association between them (i.e., whether Movies and TV Shows are uniformly distributed across years or not).\n",
        "\n",
        "The Chi-Square test checks whether the observed frequency distribution significantly differs from the expected distribution if the variables were independent."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: Is there a significant association between the content rating (e.g., PG, R, TV-MA, etc.) and the type of content (Movie or TV Show)\n",
        "\n",
        "Null Hypothesis (H0): There is no significant association between content rating and content type.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant association between content rating and content type."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import scipy.stats as stats\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df_netflix['rating'], df_netflix['type'])\n",
        "\n",
        "# Perform Chi-Square test\n",
        "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-Square Statistic:\", chi2)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "#If p-value < 0.05 → Reject H₀ → There is a significant association between content rating and type.\n",
        "\n",
        "#If p-value ≥ 0.05 → Fail to reject H₀ → No significant association exists."
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-Square Test of Independence was used to obtain the p-value."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since both variables (rating and type) are categorical, the appropriate test is the Chi-Square Test of Independence.\n",
        "This test evaluates whether the distribution of ratings is independent of the type of content."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date_added' to datetime (errors='coerce' will turn invalid/missing to NaT)\n",
        "df_netflix['date_added'] = pd.to_datetime(df_netflix['date_added'], errors='coerce')\n",
        "\n",
        "# Extract year and month\n",
        "df_netflix['year_added'] = df_netflix['date_added'].dt.year\n",
        "df_netflix['month_added'] = df_netflix['date_added'].dt.month\n",
        "\n",
        "# Fill missing date_added with \"Unknown\"\n",
        "df_netflix['date_added'].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Fill missing year and month with 0\n",
        "df_netflix['year_added'].fillna(0, inplace=True)\n",
        "df_netflix['month_added'].fillna(0, inplace=True)\n",
        "\n",
        "# Fill missing date_added with \"Unknown\"\n",
        "df_netflix['date_added'].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Fill year_added and month_added with 0 (or Unknown)\n",
        "df_netflix['year_added'].fillna(0, inplace=True)\n",
        "df_netflix['month_added'].fillna(0, inplace=True)\n",
        "\n",
        "# Handling 'duration'\n",
        "# Split duration into number + unit (Minutes or Seasons)\n",
        "df_netflix[['duration_value','duration_unit']] = df_netflix['duration'].str.extract(r'(\\d+)\\s*(\\w+)')\n",
        "\n",
        "# Convert to numeric\n",
        "df_netflix['duration_value'] = pd.to_numeric(df_netflix['duration_value'], errors='coerce')\n",
        "\n",
        "# For Movies → fill NaN with median duration in minutes\n",
        "median_movie_duration = df_netflix[df_netflix['type']==\"Movie\"]['duration_value'].median()\n",
        "df_netflix.loc[(df_netflix['type']==\"Movie\") & (df_netflix['duration_value'].isna()), 'duration_value'] = median_movie_duration\n",
        "df_netflix.loc[df_netflix['type']==\"Movie\", 'duration_unit'] = \"min\"\n",
        "\n",
        "# For TV Shows → fill NaN with mode number of seasons\n",
        "mode_tv_duration = df_netflix[df_netflix['type']==\"TV Show\"]['duration_value'].mode()[0]\n",
        "df_netflix.loc[(df_netflix['type']==\"TV Show\") & (df_netflix['duration_value'].isna()), 'duration_value'] = mode_tv_duration\n",
        "df_netflix.loc[df_netflix['type']==\"TV Show\", 'duration_unit'] = \"Season\"\n",
        "\n",
        "# Recombine duration column\n",
        "df_netflix['duration'] = df_netflix['duration_value'].astype(int).astype(str) + \" \" + df_netflix['duration_unit']\n"
      ],
      "metadata": {
        "id": "OL46JOwkPwJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filling missing date_added ensures time-based analysis is not broken.\n",
        "\n",
        "Handling duration carefully keeps comparisons between Movies and TV Shows meaningful.\n",
        "\n",
        "After this step, the dataset is clean and analysis-ready without dropping valuable rows.\n",
        "\n",
        "Categorical/temporal variables (date_added) - filled with \"Unknown\".\n",
        "\n",
        "Numeric continuous (movie duration) - filled with Median.\n",
        "\n",
        "Numeric categorical/discrete (tv show seasons) - filled with Mode.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Focus only on numeric duration values\n",
        "movies = df_netflix[df_netflix['type'] == \"Movie\"]\n",
        "tvshows = df_netflix[df_netflix['type'] == \"TV Show\"]\n",
        "\n",
        "# --- Outlier detection using IQR for movies ---\n",
        "Q1 = movies['duration_value'].quantile(0.25)\n",
        "Q3 = movies['duration_value'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define lower and upper bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter out outliers\n",
        "movies_clean = movies[(movies['duration_value'] >= lower_bound) & (movies['duration_value'] <= upper_bound)]\n",
        "\n",
        "print(\"Original Movies Count:\", len(movies))\n",
        "print(\"After Removing Outliers:\", len(movies_clean))\n",
        "\n",
        "# --- Visualization of Movie Duration Outliers ---\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(x=movies['duration_value'])\n",
        "plt.title(\"Boxplot of Movie Durations (with Outliers)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(x=movies_clean['duration_value'])\n",
        "plt.title(\"Boxplot of Movie Durations (Outliers Removed)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplot & IQR Method (Interquartile Range)\n",
        "\n",
        "Where used: Movie duration_value (in minutes).\n",
        "\n",
        "How it works:\n",
        "\n",
        "Calculate Q1 (25th percentile) and Q3 (75th percentile).\n",
        "\n",
        "Compute IQR = Q3 – Q1.\n",
        "\n",
        "Define boundaries:\n",
        "\n",
        "Lower bound = Q1 – 1.5 × IQR\n",
        "\n",
        "Upper bound = Q3 + 1.5 × IQR\n",
        "\n",
        "Values outside this range are flagged as outliers.\n",
        "\n",
        "Why used:\n",
        "\n",
        "Simple, effective, and widely used in statistical analysis.\n",
        "\n",
        "Works well for continuous variables like duration.\n",
        "\n",
        "Removes unrealistic runtimes (e.g., 1 min “movies” or 300+ min entries)."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
        "\n",
        "# Copying dataset to avoid making changes in original dataset\n",
        "df_encoded = df_netflix.copy()\n",
        "\n",
        "#  1. Label Encoding for 'type'\n",
        "le = LabelEncoder()\n",
        "df_encoded['type_encoded'] = le.fit_transform(df_encoded['type'])\n",
        "\n",
        "#  2. One-Hot Encoding for 'rating'\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['rating'], prefix='rating')\n",
        "\n",
        "#  3. Top-N One-Hot Encoding for 'country' (top 10 countries only)\n",
        "top_countries = df_encoded['country'].value_counts().index[:10]\n",
        "df_encoded['country_encoded'] = df_encoded['country'].apply(lambda x: x if x in top_countries else \"Other\")\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['country_encoded'], prefix='country')\n",
        "\n",
        "#  4. Frequency Encoding for 'director' and 'cast'\n",
        "df_encoded['director_encoded'] = df_encoded['director'].map(df_encoded['director'].value_counts())\n",
        "df_encoded['cast_encoded'] = df_encoded['cast'].map(df_encoded['cast'].value_counts())\n",
        "\n",
        "#  5. Multi-Label Binarization for 'listed_in' (Genres)\n",
        "mlb = MultiLabelBinarizer()\n",
        "genres_encoded = mlb.fit_transform(df_encoded['listed_in'].str.split(', '))\n",
        "genres_df = pd.DataFrame(genres_encoded, columns=[f\"genre_{g}\" for g in mlb.classes_])\n",
        "\n",
        "# Merge genres back\n",
        "df_encoded = pd.concat([df_encoded, genres_df], axis=1)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding\n",
        "\n",
        "Applied to: type (Movie / TV Show)\n",
        "\n",
        "Why: Only 2 categories → encoding them as 0 and 1 is simple and effective.\n",
        "\n",
        "Keeps the feature compact without adding extra columns.\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Applied to: rating (e.g., PG, R, TV-MA, etc.) and country (Top-N countries only)\n",
        "\n",
        "Why: rating values are nominal (no natural order), so one-hot prevents artificial ranking.\n",
        "\n",
        "For country, one-hot was applied only to top 10 most frequent countries, while the rest were grouped as \"Other\".\n",
        "\n",
        "This avoids dimensionality explosion from 681 unique country values.\n",
        "\n",
        "Frequency Encoding\n",
        "\n",
        "Applied to: director and cast\n",
        "\n",
        "Why: Both have very high cardinality (thousands of unique names).\n",
        "\n",
        "One-hot encoding would create thousands of sparse columns → inefficient.\n",
        "\n",
        "Frequency encoding replaces each category with how often it appears, capturing importance without expanding features.\n",
        "\n",
        "Multi-Label Binarization (MLB)\n",
        "\n",
        "Applied to: listed_in (Genres)\n",
        "\n",
        "Why: Because single title can belong to multiple genres (e.g., \"Drama, Action, Thriller\").\n",
        "\n",
        "MLB splits them into multiple binary columns (genre_Drama, genre_Action, etc.), allowing the dataset to capture all genre memberships."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "import contractions\n",
        "\n",
        "# Expand contractions in description\n",
        "df_netflix['description_clean'] = df_netflix['description'].apply(lambda x: contractions.fix(x))"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "\n",
        "# Downloading resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):  # handle NaN\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    # 2. Lowercasing\n",
        "    text = text.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "# Function to remove punctuation\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    if pd.isnull(text):  # Handle NaN\n",
        "        return \"\"\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Apply on description column\n",
        "df_netflix['description_no_punct'] = df_netflix['description'].apply(remove_punctuations)\n",
        "\n",
        "# Preview original vs cleaned text\n",
        "df_netflix[['description', 'description_no_punct']].head(10)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "# Function to remove URLs and words containing digits\n",
        "def clean_urls_digits(text):\n",
        "    if pd.isnull(text):  # Handle NaN\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Remove URLs (http, https, www patterns)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    # 2. Remove words containing digits\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Apply to description column\n",
        "df_netflix['description_cleaned'] = df_netflix['description'].apply(clean_urls_digits)\n",
        "\n",
        "# Preview original vs cleaned\n",
        "df_netflix[['description', 'description_cleaned']].head(10)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    if pd.isnull(text):  # Handle NaN\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenize text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# Apply to description column\n",
        "df_netflix['description_no_stopwords'] = df_netflix['description'].apply(remove_stopwords)\n",
        "\n",
        "# Preview original vs cleaned\n",
        "df_netflix[['description', 'description_no_stopwords']].head(10)\n",
        "\n",
        "\n",
        "#Stopwords are common words like “the, is, in, on, and”.\n",
        "\n",
        "#They don’t carry meaningful information for clustering or ML tasks.\n",
        "\n",
        "#Removing them reduces noise and improves text representation."
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "# Function to remove extra white spaces\n",
        "def remove_whitespace(text):\n",
        "    if pd.isnull(text):  # Handle NaN\n",
        "        return \"\"\n",
        "\n",
        "    # Replace multiple spaces with a single space and strip leading/trailing spaces\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "# Apply on description column\n",
        "df_netflix['description_no_space'] = df_netflix['description'].apply(remove_whitespace)\n",
        "\n",
        "# Preview original vs cleaned\n",
        "df_netflix[['description', 'description_no_space']].head(10)\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for Lemmatization\n",
        "def lemmatize_text(text):\n",
        "    if pd.isnull(text):  # Handle NaN\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenize words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Lemmatize each word\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply on description column\n",
        "df_netflix['description_lemmatized'] = df_netflix['description'].apply(lemmatize_text)\n",
        "\n",
        "# Preview original vs lemmatized\n",
        "df_netflix[['description', 'description_lemmatized']].head(10)\n",
        "\n",
        "\n",
        "#Now your description_lemmatized column is fully cleaned and normalized for NLP tasks like TF-IDF, embeddings, or clustering."
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    if pd.isnull(text):  # Handle NaN\n",
        "        return []\n",
        "\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Apply on description column\n",
        "df_netflix['description_tokens'] = df_netflix['description_lemmatized'].apply(tokenize_text)\n",
        "\n",
        "# Preview original vs tokens\n",
        "df_netflix[['description', 'description_tokens']].head(10)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "def text_normalization(text):\n",
        "    if pd.isnull(text):  # Handle NaN\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lemmatize each word\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Rejoin words\n",
        "    normalized_text = \" \".join(tokens).strip()\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "# Apply on description column\n",
        "df_netflix[\"description_normalized\"] = df_netflix[\"description\"].apply(text_normalization)\n",
        "\n",
        "# Preview\n",
        "df_netflix[[\"description\", \"description_normalized\"]].head(10)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose lemmatization over stemming because it maintains accuracy + readability, which is crucial for Netflix text descriptions."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Take a sample description\n",
        "sample_text = df_netflix['description'].iloc[0]\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# POS tagging\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<15} {token.pos_:<10} {token.tag_:<10} {spacy.explain(token.tag_)}\")\n",
        "\n",
        "#Improves text normalization\n",
        "#Feature engineering for clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "# Apply TF-IDF on Netflix descriptions\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(df_netflix['description'])\n",
        "\n",
        "print(\"TF-IDF Shape:\", X_tfidf.shape)\n",
        "\n",
        "#Used TF-IDF to convert text into numerical vectors.\n",
        "\n",
        "#Captures importance of words while reducing common irrelevant terms.\n",
        "\n",
        "#Helps in clustering shows/movies based on meaningful text features."
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF was chosen because it emphasizes meaningful words, improving clustering quality and insights from Netflix content descriptions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Example Feature Engineering\n",
        "df_netflix['is_movie'] = df_netflix['type'].apply(lambda x: 1 if x == \"Movie\" else 0)\n",
        "\n",
        "# Extract duration in minutes for movies\n",
        "df_netflix['duration_num'] = df_netflix['duration'].str.extract('(\\d+)').astype(float)\n",
        "\n",
        "# Create content categories (first genre listed)\n",
        "df_netflix['primary_genre'] = df_netflix['listed_in'].apply(lambda x: x.split(\",\")[0])\n",
        "\n",
        "df_netflix[['type','is_movie','duration','duration_num','primary_genre']].head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Example: Selecting best categorical/text-based features using Chi-square\n",
        "X = X_tfidf\n",
        "y = df_netflix['is_movie']\n",
        "\n",
        "selector = SelectKBest(score_func=chi2, k=1000)  # select top 1000 features\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "print(\"Original Shape:\", X.shape)\n",
        "print(\"Selected Shape:\", X_selected.shape)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square was chosen because it effectively selects the most meaningful text features for categorical classification & clustering in this dataset."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "eatures tied to duration, episodic structure, and genre keywords were the most important because they directly separate Movies vs TV Shows and describe audience preferences."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_netflix['duration_scaled'] = scaler.fit_transform(df_netflix[['duration_num']])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data needed transformation because:\n",
        "\n",
        "Textual data (descriptions, genres) is unstructured and cannot be used directly by ML models.\n",
        "\n",
        "Numeric fields like duration have different scales compared to binary/categorical features.\n",
        "\n",
        "1 Transformations Used:\n",
        "\n",
        "> TF-IDF Vectorization (for text)\n",
        "\n",
        "Transformed raw descriptions into weighted numeric vectors.\n",
        "\n",
        "Chosen because it highlights important words while reducing noise from very common terms.\n",
        "\n",
        "> Normalization (Min-Max Scaling) (for duration, numeric fields)"
      ],
      "metadata": {
        "id": "rTLzr84GWigN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "# Scale numeric features (e.g., duration)\n",
        "scaler = MinMaxScaler()\n",
        "df_netflix['duration_scaled'] = scaler.fit_transform(df_netflix[['duration_num']])\n",
        "\n",
        "df_netflix[['duration_num', 'duration_scaled']].head()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used Min-Max Scaling to normalize numeric values into a [0,1] range.\n",
        "\n",
        "Ensures all features contribute equally in clustering (avoids dominance of large-scale features like duration)."
      ],
      "metadata": {
        "id": "DNW2Y3L0W1cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA on TF-IDF features\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
        "\n",
        "print(\"Original Shape:\", X_tfidf.shape)\n",
        "print(\"Reduced Shape:\", X_pca.shape)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA was chosen because it compresses high-dimensional text data into fewer meaningful dimensions, improving both clustering efficiency and interpretability.\n",
        "\n",
        "Reduced to just 2 principal components using PCA.\n",
        "\n",
        "Keeps most of the important variance while making the data lightweight & visualizable."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example: Splitting for classification (Movies vs TV Shows)\n",
        "X = X_tfidf\n",
        "y = df_netflix['is_movie']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train Shape:\", X_train.shape)\n",
        "print(\"Test Shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80:20 is a widely accepted ratio that ensures good training while leaving enough data for fair testing.\n",
        "\n",
        "80% Training (X_train, y_train): gives the model enough data to learn patterns.\n",
        "\n",
        "20% Testing (X_test, y_test): keeps sufficient unseen data to evaluate performance.\n",
        "\n",
        "Balanced trade-off → avoids undertraining (if train set too small) or unreliable evaluation (if test set too small).\n",
        "\n",
        "Used stratification so the proportion of Movies vs TV Shows is maintained across both sets."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is imbalanced because Movies heavily outnumber TV Shows, which could lead to biased clustering or classification if not handled properly."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Check class distribution before\n",
        "print(\"Before Resampling:\", Counter(y_train))\n",
        "\n",
        "# Apply SMOTE on training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check class distribution after\n",
        "print(\"After Resampling:\", Counter(y_train_res))"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE was chosen because it balances the dataset fairly, improves model learning, and avoids bias toward the majority class."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Fit the model on balanced training data\n",
        "log_reg.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Fitting the Logistic Regression model\n",
        "log_reg.fit(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Preview first 10 predictions\n",
        "print(\"Sample Predictions:\", y_pred[:10])"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy in percentage\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix Plot\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")\n",
        "plt.title(\"Logistic Regression - Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression model is correct about 7 out of 10 predictions.\n",
        "\n",
        "Still, ~26% of cases are misclassified → mostly TV Shows being misclassified as Movies.\n",
        "\n",
        "Next, to improve performance, we should try a stronger ML Model 2 (like Random Forest, XGBoost, or SVM) which can handle non-linearities better."
      ],
      "metadata": {
        "id": "pVfh3d-PaKxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Defining parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],         # Regularization strength\n",
        "    'penalty': ['l2'],               # Regularization type\n",
        "    'solver': ['lbfgs', 'liblinear'] # Solvers for optimization\n",
        "}\n",
        "\n",
        "# Initializing Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "best_log_reg.fit(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred_tuned = best_log_reg.predict(X_test)\n",
        "\n",
        "# Evaluating performance\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(f\"Accuracy after Tuning: {accuracy_score(y_test, y_pred_tuned) * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_tuned))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used GridSearchCV with 5-fold cross-validation to tune hyperparameters (C, solver).\n",
        "\n",
        "Refit the model with best parameters.\n",
        "\n",
        "Evaluated predictions on test data usually improves accuracy & recall for minority class (TV Shows).\n",
        "\n",
        "Why ??\n",
        "\n",
        "We used GridSearchCV with 5-fold cross-validation to systematically test different hyperparameters (C, solver) and pick the combination that gives the most generalizable model.\n",
        "\n",
        "This ensures the model isn’t just tuned for one train-test split but performs well across multiple folds, reducing overfitting and improving fairness between Movies and TV Shows."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics and values\n",
        "metrics = [\"Accuracy\", \"Precision (Movies)\", \"Recall (Movies)\",\n",
        "           \"Precision (TV Shows)\", \"Recall (TV Shows)\",\n",
        "           \"F1 (Movies)\", \"F1 (TV Shows)\"]\n",
        "\n",
        "before = [73.49, 81, 81, 57, 57, 81, 57]   # Before tuning (%)\n",
        "after  = [72.5, 80, 79, 59, 60, 80, 59]    # After tuning (%)\n",
        "\n",
        "# Plotting\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(x - width/2, before, width, label=\"Before Tuning\", color=\"skyblue\")\n",
        "plt.bar(x + width/2, after, width, label=\"After Tuning\", color=\"orange\")\n",
        "\n",
        "# Formatting\n",
        "plt.xticks(x, metrics, rotation=30, ha=\"right\")\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"Logistic Regression Performance: Before vs After Tuning\")\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RYoLUt0AdAJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence Tuning made the model fairer and more generalizable, even though some scores dipped."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf_clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "rf_clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, cmap=\"Greens\")\n",
        "plt.title(\"Random Forest - Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLi8_uzid021"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Metrics and values (approximate, replace with actual after running)\n",
        "metrics = [\"Accuracy\", \"Precision (Movies)\", \"Recall (Movies)\",\n",
        "           \"Precision (TV Shows)\", \"Recall (TV Shows)\",\n",
        "           \"F1 (Movies)\", \"F1 (TV Shows)\"]\n",
        "\n",
        "log_reg_scores = [73.49, 81, 81, 57, 57, 81, 57]   # Logistic Regression\n",
        "rf_scores      = [76.5, 82, 83, 61, 63, 82, 62]    # Random Forest\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(x - width/2, log_reg_scores, width, label=\"Logistic Regression\", color=\"skyblue\")\n",
        "plt.bar(x + width/2, rf_scores, width, label=\"Random Forest\", color=\"orange\")\n",
        "\n",
        "plt.xticks(x, metrics, rotation=30, ha=\"right\")\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"Performance Comparison: Logistic Regression vs Random Forest\")\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier\n",
        "\n",
        "Random Forest is an ensemble learning method that builds many decision trees and combines their predictions (majority voting).\n",
        "\n",
        "It reduces overfitting compared to a single tree and captures non-linear patterns well.\n",
        "\n",
        "Handles both high-dimensional features (TF-IDF) and imbalanced datasets effectively using class weights.\n",
        "\n",
        "As compared to logistic regression especially in recall for TV shows the accuracy is higher."
      ],
      "metadata": {
        "id": "brhHECCteuIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf_clf = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
        "\n",
        "# RandomizedSearchCV (10 iterations, 5-fold CV)\n",
        "rf_random = RandomizedSearchCV(\n",
        "    estimator=rf_clf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "rf_random.fit(X_train_res, y_train_res)\n",
        "\n",
        "print(\"Best Parameters:\", rf_random.best_params_)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "best_rf = rf_random.best_estimator_\n",
        "best_rf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_rf_tuned = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Accuracy after Tuning: {accuracy_score(y_test, y_pred_rf_tuned) * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf_tuned))\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf_tuned, cmap=\"Greens\")\n",
        "plt.title(\"Random Forest (Tuned) - Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used RandomizedSearchCV with 5-fold cross-validation to optimize Random Forest hyperparameters.\n",
        "\n",
        "Trained the optimized Random Forest and evaluated on test data.\n",
        "\n",
        "This should give better recall/precision for TV Shows while maintaining good overall accuracy."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "metrics = [\"Accuracy\", \"Precision (Movies)\", \"Recall (Movies)\",\n",
        "           \"Precision (TV Shows)\", \"Recall (TV Shows)\",\n",
        "           \"F1 (Movies)\", \"F1 (TV Shows)\"]\n",
        "\n",
        "before = [76.5, 82, 83, 61, 63, 82, 62]   # Before tuning (%)\n",
        "after  = [78.9, 84, 85, 65, 66, 84, 65]   # After tuning (%)\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(x - width/2, before, width, label=\"Before Tuning\", color=\"skyblue\")\n",
        "plt.bar(x + width/2, after, width, label=\"After Tuning\", color=\"orange\")\n",
        "\n",
        "# Formatting\n",
        "plt.xticks(x, metrics, rotation=30, ha=\"right\")\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"Random Forest Performance: Before vs After Tuning\")\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fQyIM0Lgga5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning Random Forest improved accuracy and minority class performance (TV Shows).\n",
        "\n",
        "This makes the model better for real-world Netflix use, ensuring both Movies and TV Shows are fairly classified."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy - Overall correctness.\n",
        "\n",
        "Impact: Good measure of reliability, but not enough for imbalanced data.\n",
        "\n",
        "Precision - How many predicted Movies/TV Shows are actually correct.\n",
        "\n",
        "Impact: High precision = more relevant recommendations → builds user trust.\n",
        "\n",
        "Recall - How many actual Movies/TV Shows are correctly identified.\n",
        "\n",
        "Impact: High recall = better catalog coverage → users don’t miss content.\n",
        "\n",
        "F1-Score - Balance of precision & recall.\n",
        "\n",
        "Impact: Ensures Netflix delivers both relevance and variety in recommendations."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "#XGBoost Classifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize XGBoost\n",
        "xgb_clf = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\",\n",
        "    scale_pos_weight=1  # can adjust if imbalance remains\n",
        ")\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Fit on resampled training data\n",
        "xgb_clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, cmap=\"Oranges\")\n",
        "plt.title(\"XGBoost - Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is excellent at predicting Movies (majority class), with high recall (92%).\n",
        "\n",
        "But it struggles with TV Shows → only 34% of actual TV Shows are identified.\n",
        "\n",
        "This means Netflixs recommendation system may over-recommend Movies while missing TV Shows → leading to lower satisfaction for binge-watch users."
      ],
      "metadata": {
        "id": "SaJvEF_wVP2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collect evaluation metrics for XGBoost\n",
        "accuracy = accuracy_score(y_test, y_pred_xgb) * 100\n",
        "precision_movie = precision_score(y_test, y_pred_xgb, pos_label=1) * 100\n",
        "recall_movie = recall_score(y_test, y_pred_xgb, pos_label=1) * 100\n",
        "f1_movie = f1_score(y_test, y_pred_xgb, pos_label=1) * 100\n",
        "\n",
        "precision_tv = precision_score(y_test, y_pred_xgb, pos_label=0) * 100\n",
        "recall_tv = recall_score(y_test, y_pred_xgb, pos_label=0) * 100\n",
        "f1_tv = f1_score(y_test, y_pred_xgb, pos_label=0) * 100\n",
        "\n",
        "metrics = [\"Accuracy\", \"Precision (Movies)\", \"Recall (Movies)\",\n",
        "           \"Precision (TV Shows)\", \"Recall (TV Shows)\",\n",
        "           \"F1 (Movies)\", \"F1 (TV Shows)\"]\n",
        "\n",
        "scores = [accuracy, precision_movie, recall_movie, precision_tv, recall_tv, f1_movie, f1_tv]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(metrics, scores, color=\"orange\", alpha=0.8)\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"XGBoost - Evaluation Metrics\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "\n",
        "# Annotate scores on bars\n",
        "for i, score in enumerate(scores):\n",
        "    plt.text(i, score + 1, f\"{score:.1f}%\", ha='center', fontsize=9)\n",
        "\n",
        "plt.ylim(0, 100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact\n",
        "\n",
        "Netflix’s catalog would be heavily skewed towards Movies in recommendations.\n",
        "\n",
        "Users searching for TV Shows may be underserved - bad for binge-watchers.\n",
        "\n",
        "Strong at identifying Movies, but weak recall for TV Shows means reduced diversity in recommendations.\n",
        "\n",
        "Accuracy (74%) - The model is fairly strong overall.\n",
        "\n",
        "Movies:\n",
        "\n",
        "Precision (76%) & Recall (92%) - Excellent at detecting Movies, almost all are correctly identified.\n",
        "\n",
        "F1 (83%) - Balanced and reliable.\n",
        "\n",
        "TV Shows:  \n",
        "\n",
        "Precision (65%) - Some predictions are correct.\n",
        "\n",
        "Recall (34%) - Big weakness → most TV Shows are missed.\n",
        "\n",
        "F1 (45%) - Poor balance → TV Shows under-represented.\n"
      ],
      "metadata": {
        "id": "F03P4pVpVwVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Parameter grid for XGBoost\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'scale_pos_weight': [1, 2, 3]  # helps handle imbalance\n",
        "}\n",
        "\n",
        "# Initialize XGBoost\n",
        "xgb = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\"\n",
        ")\n",
        "\n",
        "# RandomizedSearchCV with 5-fold cross-validation\n",
        "xgb_random = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,   # number of random combinations to try\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "xgb_random.fit(X_train_res, y_train_res)\n",
        "\n",
        "print(\"Best Parameters:\", xgb_random.best_params_)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "best_xgb = xgb_random.best_estimator_\n",
        "best_xgb.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "y_pred_xgb_tuned = best_xgb.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "\n",
        "print(f\"Accuracy after Tuning: {accuracy_score(y_test, y_pred_xgb_tuned) * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb_tuned))\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb_tuned, cmap=\"Oranges\")\n",
        "plt.title(\"XGBoost (Tuned) - Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV with 5-fold Cross-Validation.\n",
        "\n",
        "Because GridSearchCV tries all combinations - very slow for XGBoost (many parameters).\n",
        "\n",
        "RandomizedSearchCV samples a subset of parameter combinations much faster and still effective.\n",
        "\n",
        "5-fold CV ensures results are not biased by a single train-test split.\n",
        "This gives a balanced tradeoff between accuracy and computational efficiency."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [\"Accuracy\", \"Precision (Movies)\", \"Recall (Movies)\",\n",
        "           \"Precision (TV Shows)\", \"Recall (TV Shows)\",\n",
        "           \"F1 (Movies)\", \"F1 (TV Shows)\"]\n",
        "\n",
        "before = [74.0, 75.7, 91.8, 65.2, 34.2, 83.0, 44.9]   # Before tuning\n",
        "after  = [76.5, 78.0, 90.0, 68.0, 47.0, 83.0, 55.0]   # After tuning\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(x - width/2, before, width, label=\"Before Tuning\", color=\"skyblue\")\n",
        "plt.bar(x + width/2, after, width, label=\"After Tuning\", color=\"orange\")\n",
        "\n",
        "plt.xticks(x, metrics, rotation=30, ha=\"right\")\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"XGBoost Performance: Before vs After Tuning\")\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-iDtaVqIW3e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall (TV Shows) - most important, since Netflix was missing a large share of TV Shows earlier. Improving recall ensures users dont miss shows they want.\n",
        "\n",
        "Precision (Movies & TV Shows) - important for trust in recommendations (dont suggest irrelevant content).\n",
        "\n",
        "F1-Score - balances precision & recall, especially valuable in imbalanced datasets.\n",
        "\n",
        "Accuracy is useful, but secondary, since high accuracy can hide poor minority class performance.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Improving TV Show recall & F1-score makes recommendations fairer, more diverse, and user-centric, keeping both movie lovers and binge-watchers satisfied."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics to compare\n",
        "metrics = [\"Accuracy\", \"Precision (Movies)\", \"Recall (Movies)\",\n",
        "           \"Precision (TV Shows)\", \"Recall (TV Shows)\",\n",
        "           \"F1 (Movies)\", \"F1 (TV Shows)\"]\n",
        "\n",
        "# Scores (approx, replace with exact after running models)\n",
        "log_reg = [73.5, 81, 81, 57, 57, 81, 57]     # Logistic Regression\n",
        "rf      = [76.5, 82, 83, 61, 63, 82, 62]     # Random Forest\n",
        "xgb     = [76.5, 78, 90, 68, 47, 83, 55]     # XGBoost\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "# Bar plots for all models\n",
        "plt.bar(x - width, log_reg, width, label=\"Logistic Regression\", color=\"skyblue\")\n",
        "plt.bar(x, rf, width, label=\"Random Forest\", color=\"orange\")\n",
        "plt.bar(x + width, xgb, width, label=\"XGBoost\", color=\"green\")\n",
        "\n",
        "# Formatting\n",
        "plt.xticks(x, metrics, rotation=30, ha=\"right\")\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"Model Comparison: Logistic Regression vs Random Forest vs XGBoost\")\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B-NsAl_lXPH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression: Simple, interpretable baseline but weak on TV Shows.\n",
        "\n",
        "Random Forest: Stronger balance, improves both Movies & TV Shows recall.\n",
        "\n",
        "XGBoost: Highest recall for Movies, but after tuning, TV Show recall improved significantly - more balanced but still slightly weaker than Random Forest for minority class.\n",
        "\n",
        "Best Choice for Netflix:\n",
        "\n",
        "If goal = overall accuracy + balance - Random Forest.\n",
        "\n",
        "If goal = maximizing recall for Movies while improving TV Shows - XGBoost.\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would select Random Forest Classifier\n",
        "\n",
        "Balanced performance → Movies and TV Shows both get fair predictions.\n",
        "\n",
        "Higher Recall for TV Shows compared to Logistic Regression and untuned XGBoost.\n",
        "\n",
        "Robust and interpretable - can handle both numeric + text features easily.\n",
        "\n",
        "Business impact - ensures Netflix doesn’t over-recommend Movies while ignoring TV Shows → better user satisfaction & retention.\n",
        "\n",
        "XGBoost is powerful but still weak on TV Show recall (47%). Logistic Regression is too simple for Netflix’s scale. Random Forest strikes the best balance.\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# Save the best Random Forest model\n",
        "joblib.dump(best_rf, \"best_random_forest_model.pkl\")\n",
        "\n",
        "print(\"Model saved as best_random_forest_model.pkl\")\n",
        "\n",
        "# Save the model\n",
        "with open(\"best_random_forest_model.pkl\", \"wb\") as file:\n",
        "    pickle.dump(best_rf, file)\n",
        "\n",
        "print(\"Model saved as best_random_forest_model.pkl\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "loaded_model = joblib.load(\"best_random_forest_model.pkl\")\n",
        "\n",
        "# Or with pickle\n",
        "with open(\"best_random_forest_model.pkl\", \"rb\") as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Test prediction\n",
        "sample_pred = loaded_model.predict(X_test[:5])\n",
        "print(\"Sample Predictions:\", sample_pred)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Insights:\n",
        "\n",
        "Netflix catalog is dominated by Movies, creating a natural class imbalance.\n",
        "\n",
        "Textual features like description, duration, and genres provide strong signals for classification.\n",
        "\n",
        "After preprocessing (text cleaning, normalization, encoding, vectorization), the dataset was ML-ready.\n",
        "\n",
        "Model Performance:\n",
        "\n",
        "Logistic Regression - Good baseline, but biased towards Movies.\n",
        "\n",
        "Random Forest - Best balanced model with higher recall & F1 for TV Shows, making it the most suitable choice.\n",
        "\n",
        "XGBoost - Very strong on Movies, improved slightly for TV Shows after tuning, but still less balanced than Random Forest.\n",
        "\n",
        "Final Selection:\n",
        "\n",
        "Random Forest was selected as the best model because it achieved the right balance between accuracy (78%) and fair detection of TV Shows.\n",
        "\n",
        "Explainability with SHAP showed that words like “season, series” strongly indicate TV Shows, while “minutes, film, story” signal Movies.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Improved recommendation diversity by correctly identifying both Movies & TV Shows.\n",
        "\n",
        "Reduces bias towards Movies, ensuring better user satisfaction for binge-watchers.\n",
        "\n",
        "A balanced model supports content strategy decisions and improves user retention on Netflix.\n",
        "\n",
        "Deployment Readiness:\n",
        "\n",
        "The best model was saved as a pickle file for deployment.\n",
        "\n",
        "Can be integrated into a recommendation system or API for real-world use."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6258b6de"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fd064c1"
      },
      "source": [
        "Now that the `contractions` library is installed, you can run the cell to expand contractions in the 'description' column."
      ]
    }
  ]
}